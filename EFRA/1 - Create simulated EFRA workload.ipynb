{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd20816-09b3-48ae-85fd-ed366e227a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287f86a-cd3f-4157-98d1-aaf437da5ab9",
   "metadata": {},
   "source": [
    "### Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7f237-2c88-4b3d-a16e-954fe1d2dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_by_probability_deterministic(df: pd.DataFrame,\n",
    "                                        prob_col: str = \"prob\",\n",
    "                                        total_rows: int = 1000) -> pd.DataFrame:\n",
    "\n",
    "    # Extract the Series containing the probabilities\n",
    "    p = pd.to_numeric(df[prob_col], errors=\"coerce\")\n",
    "\n",
    "    # Determine the number of times each original row will be repeated in the final dataframe. Put this info in 'scaled'.\n",
    "    # NOTE: we are ignoring small rounding errors, which might occur if the probabilites are expressed with more than two decimal values.\n",
    "    #       Hence, we are OK if the final dataframe has slightly less/more rows than 'total_rows'\n",
    "    scaled = (p * int(total_rows)).astype(int)\n",
    "\n",
    "    # Repeat the original rows by (1) repeating the original index entries, and then (2) using the repeated index entries to repeatedly\n",
    "    # select the original rows.\n",
    "    repeated_index = df.index.repeat(scaled)\n",
    "    return df.loc[repeated_index].drop(columns = prob_col).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09de0e-38ab-4200-8a28-461fe46f0f59",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c299977-7a3c-4a0e-b381-5ad97b91b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_specs = pd.read_csv('./specs_2/specs workload EFRA.csv')\n",
    "orig_specs.info()\n",
    "print(f\"Sum of probabilities: {orig_specs['prob'].sum()}\")\n",
    "display(orig_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ab500-5e2c-4474-90fb-4e694c68ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic workload.\n",
    "simulated_workload = expand_by_probability_deterministic(orig_specs, 'prob', 1000)\n",
    "\n",
    "# Shuffle the pods.\n",
    "simulated_workload = simulated_workload.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Rename the pods.\n",
    "simulated_workload['name'] = 'openb-pod-' + simulated_workload.index.astype(str).str.zfill(4)\n",
    "\n",
    "simulated_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742a99f5-3820-4e81-996c-243bbae2e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the remaining columns expected by the simulator. \n",
    "# Use constant values -- they will be ignored during the simulations anyways...\n",
    "\n",
    "simulated_workload.loc[:, ['qos', 'pod_phase', 'creation_time', 'deletion_time', 'scheduled_time']] = 'LS', 'Running', 0, 20, 10\n",
    "simulated_workload.to_csv('./openb_pod_list_default.csv', index = False)\n",
    "simulated_workload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
