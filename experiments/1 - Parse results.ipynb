{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43262eb9-390c-4c07-a2aa-e09656ed8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4902ea0-d27a-494d-bdd1-166f1b5c66a5",
   "metadata": {},
   "source": [
    "# Parse results from logs. Retrieve power consumption, fragmentation, and task scheduling information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7101db-67dc-4a4f-ac53-8eb500f90d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries where the results will be stored.\n",
    "df_pwr_dict = {}\n",
    "df_frag_dict = {}\n",
    "df_sched_pod_dict = {}\n",
    "\n",
    "DATADIR = \"./2024_0606\"\n",
    "data = Path(DATADIR)\n",
    "\n",
    "fileDirs = sorted([x for x in data.iterdir() if x.is_dir()])\n",
    "for fdir in fileDirs:\n",
    "    df_pwr_dict[fdir.name] = {}\n",
    "    df_frag_dict[fdir.name] = {}\n",
    "    df_sched_pod_dict[fdir.name] = {}\n",
    "\n",
    "    policyDirs = sorted([x for x in fdir.iterdir() if x.is_dir()])\n",
    "    for pdir in policyDirs:\n",
    "        print(f\"Processing data for set of experiments: {fdir.name}.{pdir.name}\")\n",
    "        \n",
    "        tuneDirs = sorted([x for x in pdir.iterdir() if x.is_dir()])\n",
    "        for tdir in tuneDirs:\n",
    "            seedDirs = sorted([x for x in tdir.iterdir() if x.is_dir()])\n",
    "            for sdir in seedDirs:\n",
    "                pwrfile = sdir / 'analysis_pwr.csv'\n",
    "                grep = sdir / 'analysis_grep.out'\n",
    "                alloc = sdir / 'analysis_allo.csv'\n",
    "                schedfile = sdir / 'analysis_cdol.csv'\n",
    "                fragfile = sdir / 'analysis_frag.csv'\n",
    "\n",
    "                # Retrieve the total GPU cluster capacity (in millis).\n",
    "                try:\n",
    "                    with open(grep, 'r') as file:\n",
    "                        content = file.read()  # Read the entire file into a string\n",
    "                        # Find the position of 'allocation: ' in the content\n",
    "                        start_idx = content.find('MilliGpu: ')\n",
    "                        if start_idx != -1:\n",
    "                            # Find the end of the line\n",
    "                            end_idx = content.find('\\n', start_idx)\n",
    "                            # Extract the allocation part\n",
    "                            selection = content[start_idx:end_idx]\n",
    "                            # Extract the integer part by splitting the string\n",
    "                            allocation_value = int(selection.split(\"/\")[1].split(\")\")[0])\n",
    "                        else:\n",
    "                            raise Exception(\"MilliGpu cluster info not found. Error!\\n\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR grep analysis: %s\\n\" % (e))\n",
    "\n",
    "                # print(f\"Total GPU cluster capacity: {allocation_value}\\n\")\n",
    "\n",
    "                \n",
    "                \n",
    "                ### Collect telemetry about the GPU workload (in millis) that the cluster has received. ###\n",
    "                try:\n",
    "                    df_allo = pd.read_csv(alloc)\n",
    "                    df_allo.rename(columns = lambda x: x.split('-')[-1], inplace=True)\n",
    "                    cum_gpu_allo = df_allo.loc[:, 'arrived_gpu_milli'] / allocation_value\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR alloc analysis: %s\\n\" % (e))\n",
    "\n",
    "                # print(df_allo)\n",
    "                # sys.exit() # DEBUG!\n",
    "                \n",
    "\n",
    "                \n",
    "                # Set up the index for GPU requests received by the cluster.\n",
    "                new_index = np.arange(0, 1.005, 0.005)\n",
    "\n",
    "                ### Collect telemetry about power consumption. ###\n",
    "                try:\n",
    "                    df_pwr = pd.read_csv(pwrfile)\n",
    "                    df_pwr.rename(columns = lambda x: x.split('-')[-1], inplace=True)\n",
    "                    df_pwr[\"cumulative_workload\"] = cum_gpu_allo\n",
    "                    df_pwr.set_index(\"cumulative_workload\", inplace = True)\n",
    "\n",
    "                    # Remove rows with duplicated index entries, keeping only the first entry for each group of duplicates.\n",
    "                    # Then, add the entries in new_index in df_pwr's existing index via a union.\n",
    "                    # Then, reindex and interpolate the missing values.\n",
    "                    # Finally, keep only the rows whose index entries are present in new_index (i.e., the regularly spaced ones).\n",
    "                    df_pwr = df_pwr[~df_pwr.index.duplicated(keep='first')]\n",
    "                    df_pwr = df_pwr.reindex(df_pwr.index.union(new_index)).interpolate(method='linear').ffill().bfill()\n",
    "                    df_pwr = df_pwr.loc[new_index]\n",
    "                    if df_pwr.isna().any().any(): \n",
    "                        # print(df_pwr[df_pwr.isna().any(axis=1)])\n",
    "                        raise Exception(\"dataframe contains NaNs!\\n\")\n",
    "                        \n",
    "                    df_pwr_dict[fdir.name].setdefault(pdir.name, list()).append(df_pwr)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR power analysis: %s\\n\" % (e))\n",
    "\n",
    "                # print(df_pwr)\n",
    "                # sys.exit() # DEBUG!\n",
    "\n",
    "\n",
    "\n",
    "                ### Collect telemetry about power consumption. ###\n",
    "                try:\n",
    "                    df_frag = pd.read_csv(fragfile)\n",
    "                    df_frag.rename(columns = lambda x: x.split('-')[-1], inplace=True)\n",
    "                    df_frag = df_frag[[\"origin_ratio\"]]\n",
    "                    df_frag[\"cumulative_workload\"] = cum_gpu_allo\n",
    "                    df_frag.set_index(\"cumulative_workload\", inplace = True)\n",
    "\n",
    "                    # Remove rows with duplicated index entries, keeping only the first entry for each group of duplicates.\n",
    "                    # Then, add the entries in new_index in df_pwr's existing index via a union.\n",
    "                    # Then, reindex and interpolate the missing values.\n",
    "                    # Finally, keep only the rows whose index entries are present in new_index (i.e., the regularly spaced ones).\n",
    "                    df_frag = df_frag[~df_frag.index.duplicated(keep='first')]\n",
    "                    df_frag = df_frag.reindex(df_frag.index.union(new_index)).interpolate(method='linear').ffill().bfill()\n",
    "                    df_frag = df_frag.loc[new_index]\n",
    "                    if df_frag.isna().any().any(): \n",
    "                        # print(df_frag[df_frag.isna().any(axis=1)])\n",
    "                        raise Exception(\"dataframe contains NaNs!\\n\")\n",
    "                        \n",
    "                    df_frag_dict[fdir.name].setdefault(pdir.name, list()).append(df_frag)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"ERROR fragmentation analysis: %s\\n\" % (e))\n",
    "\n",
    "                #print(df_frag)\n",
    "                #sys.exit() # DEBUG!\n",
    "                \n",
    "\n",
    "                \n",
    "                ### Collect telemetry about pods that the cluster failed to schedule. ###\n",
    "                try:\n",
    "                    df_sched_pod = pd.read_csv(schedfile)\n",
    "                    df_sched_pod.rename(columns = lambda x: x.split('-')[-1], inplace=True)\n",
    "                    df_sched_pod = df_sched_pod[['event']]\n",
    "                    df_sched_pod[\"issued_pods\"] = df_sched_pod.index + 1\n",
    "                    df_sched_pod[\"cumulative_workload\"] = cum_gpu_allo\n",
    "                    df_sched_pod['event'] = 1 * (df_sched_pod['event'] == 'failed')\n",
    "                    df_sched_pod['event'] = df_sched_pod['event'].cumsum()\n",
    "                    df_sched_pod.rename(columns = {'event' : 'failed_pods_cumsum'}, inplace = True)\n",
    "                    df_sched_pod['arrived_gpu_milli'] = df_allo['arrived_gpu_milli']\n",
    "                    df_sched_pod['used_gpu_milli'] = df_allo['used_gpu_milli']\n",
    "                    df_sched_pod['successful_pods'] = df_sched_pod[\"issued_pods\"] - df_sched_pod[\"failed_pods_cumsum\"]\n",
    "                    df_sched_pod.set_index(\"cumulative_workload\", inplace = True)\n",
    "\n",
    "                    df_sched_pod = df_sched_pod[~df_sched_pod.index.duplicated(keep='first')]\n",
    "                    df_sched_pod = df_sched_pod.reindex(df_sched_pod.index.union(new_index)).interpolate(method='linear').ffill().bfill()\n",
    "                    df_sched_pod = df_sched_pod.loc[new_index]\n",
    "                    if df_sched_pod.isna().any().any(): \n",
    "                        raise Exception(\"dataframe contains NaNs!\\n\")\n",
    "                    \n",
    "                    df_sched_pod_dict[fdir.name].setdefault(pdir.name, list()).append(df_sched_pod)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"ERROR scheduling analysis: %s\\n\" % (e))\n",
    "\n",
    "                # print(df_sched_pod)\n",
    "                # sys.exit() # DEBUG!\n",
    "\n",
    "\n",
    "# display(df_pwr_dict.keys())\n",
    "# display(df_pwr_dict)\n",
    "# display(df_sched_pod_dict.keys())\n",
    "# display(df_sched_pod_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64894368-5825-48ad-9505-7fc1e301c4fa",
   "metadata": {},
   "source": [
    "### Compute the average power consumption, fragmentation, and number of failed plugins within each score plugin's set of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092758e9-9c8f-4fe4-9a28-ac38b4bb52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of competitors to exclude to avoid cluttering the plots too much.\n",
    "set_remove_competitors = {'01-Random',\n",
    "                          #'02-DotProd',\n",
    "                          #'03-GpuClustering',\n",
    "                          #'04-GpuPacking',\n",
    "                          #'05-BestFit',\n",
    "                          '08-PWR_500_FGD_500', \n",
    "                          '13-PWR_25_FGD_975', \n",
    "                          '09-PWR_300_FGD_700', \n",
    "                          '10-PWR_200_FGD_800',}\n",
    "\n",
    "\n",
    "# Compute the average power consumption for each score plugin.\n",
    "dict_pwr_final_res = {}\n",
    "for k in df_pwr_dict.keys() :\n",
    "    dict_pwr_final_res[k] = {}\n",
    "    for k2 in df_pwr_dict[k].keys() :\n",
    "        if k2 in set_remove_competitors : continue\n",
    "        print(f\"Computing cluster power consumption mean for level ({k},{k2}) ({len(df_pwr_dict[k][k2])} repetitions found)\")\n",
    "        dict_pwr_final_res[k][k2] = sum(df_pwr_dict[k][k2]) / len(df_pwr_dict[k][k2])\n",
    "\n",
    "\n",
    "# Compute the average fragmentation for each score plugin.\n",
    "dict_frag_final_res = {}\n",
    "for k in df_frag_dict.keys() :\n",
    "    dict_frag_final_res[k] = {}\n",
    "    for k2 in df_frag_dict[k].keys() :\n",
    "        if k2 in set_remove_competitors : continue\n",
    "        print(f\"Computing fragmentation mean for level ({k},{k2}) ({len(df_frag_dict[k][k2])} repetitions found)\")\n",
    "        dict_frag_final_res[k][k2] = sum(df_frag_dict[k][k2]) / len(df_frag_dict[k][k2])\n",
    "\n",
    "\n",
    "# Compute the average number of failed pod for each score plugin.\n",
    "dict_sched_final_res = {}\n",
    "for k in df_sched_pod_dict.keys() :\n",
    "    dict_sched_final_res[k] = {}\n",
    "    for k2 in df_sched_pod_dict[k].keys() :\n",
    "        if k2 in set_remove_competitors : continue\n",
    "        print(f\"Computing mean # of pods that have failed to schedule for level ({k},{k2}) ({len(df_sched_pod_dict[k][k2])} repetitions found)\")\n",
    "        dict_sched_final_res[k][k2] = sum(df_sched_pod_dict[k][k2]) / len(df_sched_pod_dict[k][k2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8c9d6-0836-4f2c-8d15-7a828af424fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dict_pwr_final_res.keys())\n",
    "# display(dict_pwr_final_res['openb_pod_list_default']['01-Random'])\n",
    "# display(dict_sched_final_res.keys())\n",
    "# display(dict_sched_final_res['openb_pod_list_default']['01-Random'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050460d7-99ac-4705-8ffa-558f43de58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautify the names of the policies for the plots.\n",
    "for k_level in dict_pwr_final_res.keys() :\n",
    "    dict_pwr_final_res[k_level] = {key.split('-')[1]: value for key, value in dict_pwr_final_res[k_level].items()}\n",
    "    dict_frag_final_res[k_level] = {key.split('-')[1]: value for key, value in dict_frag_final_res[k_level].items()}\n",
    "    dict_sched_final_res[k_level] = {key.split('-')[1]: value for key, value in dict_sched_final_res[k_level].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdafe4a-4a43-4c2d-bb53-fba969e9b64e",
   "metadata": {},
   "source": [
    "# Measuring GPU efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9fa61-1a30-488b-833f-c5bb0a34b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_efficiency = {}\n",
    "for level in dict_pwr_final_res.keys() :\n",
    "    \n",
    "    dict_efficiency[level] = {}\n",
    "    max_energy_efficiency = 0\n",
    "    \n",
    "    # Compute energy and usage efficiency.\n",
    "    for policy in dict_pwr_final_res[level].keys() :\n",
    "        # print(f'{level} - {policy}')\n",
    "\n",
    "        # display(dict_pwr_final_res[level][policy])\n",
    "        # display(dict_sched_final_res[level][policy])\n",
    "\n",
    "        dict_efficiency[level][policy] = pd.DataFrame(index = dict_pwr_final_res[level][policy].index)\n",
    "\n",
    "        # Usage efficiency is computed as the ratio between successfully allocated GPU resources and requested GPU resources.\n",
    "        # This comes already normalized in [0,1]\n",
    "        dict_efficiency[level][policy]['usage_efficiency'] = dict_sched_final_res[level][policy]['used_gpu_milli']/ dict_sched_final_res[level][policy]['arrived_gpu_milli']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbad72-7015-4ca9-8089-22d07e497342",
   "metadata": {},
   "source": [
    "# Save all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e2edd-b31f-44e7-8144-2f5cd74a0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_pwr_final_res.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_pwr_final_res, handle)\n",
    "\n",
    "with open('dict_frag_final_res.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_frag_final_res, handle)\n",
    "\n",
    "with open('dict_sched_final_res.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_sched_final_res, handle)\n",
    "\n",
    "with open('dict_efficiency.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_efficiency, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
